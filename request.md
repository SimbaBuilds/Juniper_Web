Large language model outputs are inherently probabilistic; this is great for simulating human or super human like intelligence, but it causes problems when you need a system to behave predicatably and reliably.  Many AI engineers have run in to the Problem of Determinism.  

Apart from wearables and health data integration, Juniper also integrates with productivity services like Gmail and Slack.  When I was testing Juniper, I asked it to draft an email to my friend - it proceeded to send the actual email.  Luckily, no harm was done apart from some embarrassment, but I had to decide what to do.  I realized the agent sending the email had 3 or so mentions of sending an email in its system prompt so, probabilistically, the LLM decided the user wanted to “send” not “draft” the email.  I went ahead and varied the system prompt, but I needed something more robust for something as important as sending emails.  I decided to add a hardcoded check: if the actual word “send” was not in the user’s most recent chat message, the agent could not send an email, Slack message, etc…  This would just be a heuristic the user will need to be notified of and reminded about.

Similar to LLM outputs, semantic search is also probabalistic.  For added determinism,  I added a robust tagging system to complement semantic search; user resources can be typed as “Memories”, “Reference”, “Notes”, and “Files”, and be tagged with 100+ unique tags.  This ensures that agents completing certain workflows always see certain resources rather than probably see them.

How has the problem of determinism manifested in your AI workflows?  Let us know.  
